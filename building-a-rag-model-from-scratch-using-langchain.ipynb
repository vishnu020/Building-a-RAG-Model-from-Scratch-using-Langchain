{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0fc28d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T22:34:30.308356Z",
     "iopub.status.busy": "2024-06-15T22:34:30.307411Z",
     "iopub.status.idle": "2024-06-15T22:34:30.312566Z",
     "shell.execute_reply": "2024-06-15T22:34:30.311540Z",
     "shell.execute_reply.started": "2024-06-15T22:34:30.308313Z"
    }
   },
   "source": [
    "# Installing the Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05be1cf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:00:03.434053Z",
     "iopub.status.busy": "2024-06-16T11:00:03.433074Z",
     "iopub.status.idle": "2024-06-16T11:00:03.438294Z",
     "shell.execute_reply": "2024-06-16T11:00:03.437364Z",
     "shell.execute_reply.started": "2024-06-16T11:00:03.434015Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install langchain-huggingface\n",
    "# !pip install langchain\n",
    "# !pip install langchain-community langchain-core\n",
    "# !pip install langchain-cohere\n",
    "# !pip install -U langchain-community faiss-cpu langchain-openai tiktoken\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422648a0",
   "metadata": {},
   "source": [
    "# Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90a76f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:18:34.248307Z",
     "iopub.status.busy": "2024-06-16T11:18:34.247409Z",
     "iopub.status.idle": "2024-06-16T11:18:34.254635Z",
     "shell.execute_reply": "2024-06-16T11:18:34.253456Z",
     "shell.execute_reply.started": "2024-06-16T11:18:34.248276Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac345d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T22:34:30.330946Z",
     "iopub.status.busy": "2024-06-15T22:34:30.330175Z",
     "iopub.status.idle": "2024-06-15T22:34:30.337987Z",
     "shell.execute_reply": "2024-06-15T22:34:30.337047Z",
     "shell.execute_reply.started": "2024-06-15T22:34:30.330914Z"
    }
   },
   "source": [
    "# Configure the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4253a9f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:00:12.712111Z",
     "iopub.status.busy": "2024-06-16T11:00:12.711061Z",
     "iopub.status.idle": "2024-06-16T11:00:12.727278Z",
     "shell.execute_reply": "2024-06-16T11:00:12.726138Z",
     "shell.execute_reply.started": "2024-06-16T11:00:12.712070Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/key-111/langchain_open_api_key.txt\",'r') as m:\n",
    "    langchain_key=m.read()\n",
    "with open(\"/kaggle/input/key-111/cohere_open_api_key.txt\",\"r\") as z:\n",
    "    cohere_api_key=z.read()\n",
    "os.environ['LANGCHAIN_API_KEY']=langchain_key\n",
    "os.environ['COHERE_API_KEY']=cohere_api_key\n",
    "with open(\"/kaggle/input/hugging-face-token/hugging_face_token.txt\",'r') as k:\n",
    "    hugging_face_token=k.read()\n",
    "os.environ['HUGGINGFACE_API_TOKEN']=hugging_face_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c140733",
   "metadata": {},
   "source": [
    "# Extracring the text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "840f1f47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:00:15.873385Z",
     "iopub.status.busy": "2024-06-16T11:00:15.872596Z",
     "iopub.status.idle": "2024-06-16T11:00:17.694370Z",
     "shell.execute_reply": "2024-06-16T11:00:17.693549Z",
     "shell.execute_reply.started": "2024-06-16T11:00:15.873354Z"
    }
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader \n",
    "reader=PdfReader(\"/kaggle/input/attention-all-you-need/1706.03762v7.pdf\")\n",
    "\n",
    "text = \"\"\n",
    "for i in range(len(reader.pages)):\n",
    "    page = reader.pages[i]\n",
    "    text_of_page = page.extract_text()\n",
    "    if text_of_page:\n",
    "        text += text_of_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b039f23",
   "metadata": {},
   "source": [
    "# Embedding model and Cohere llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "944e520c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:00:23.592547Z",
     "iopub.status.busy": "2024-06-16T11:00:23.591822Z",
     "iopub.status.idle": "2024-06-16T11:00:46.746407Z",
     "shell.execute_reply": "2024-06-16T11:00:46.745168Z",
     "shell.execute_reply.started": "2024-06-16T11:00:23.592516Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-06-16 11:00:31.555437: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-16 11:00:31.555545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-16 11:00:31.681014: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c253951f2443e18c7cd0e007d315c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6178afa39ef4daa81cd7cc735d1abc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769b6f9c63cd4d0b93705270232c7639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af7f85af08c4b95868ccc872c62b76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bbfb4fde0b40cab7072709261071f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e491f81fb3d140c582995b69d2c7ec0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f93c26e1094294bc261f6aba546223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8508807b8541456fbf790665091d3af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e54eb669f446e49029a3243aac30b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a657b3dfd4254cbe845243e7506d6bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d578e9edeeb0416daa9c6f499e949ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = ChatCohere()\n",
    "hf_embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6e819",
   "metadata": {},
   "source": [
    "# Vector-store(Faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "040f1d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:00:54.293778Z",
     "iopub.status.busy": "2024-06-16T11:00:54.293083Z",
     "iopub.status.idle": "2024-06-16T11:00:56.049465Z",
     "shell.execute_reply": "2024-06-16T11:00:56.048687Z",
     "shell.execute_reply.started": "2024-06-16T11:00:54.293745Z"
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                                chunk_overlap=200,\n",
    "                                                length_function=len,\n",
    "                                                is_separator_regex=False,)\n",
    "texts=text_splitter.create_documents([text])\n",
    "db = FAISS.from_documents(texts, hf_embeddings)\n",
    "retriever=db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "108a4d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:22:45.457969Z",
     "iopub.status.busy": "2024-06-16T11:22:45.457336Z",
     "iopub.status.idle": "2024-06-16T11:22:45.463576Z",
     "shell.execute_reply": "2024-06-16T11:22:45.462609Z",
     "shell.execute_reply.started": "2024-06-16T11:22:45.457939Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template= \"\"\"You are a bot. Your job is to provide the answer with respect to the question asked by the user.\n",
    "    You have to provide the answer based on the context provided by the user.If you dont know the answer then reply \n",
    "    sorry i dont know the answer,Don't provide the unnecessary answers Provide the answer in the best neat and clean way:\n",
    "    context: {context}\n",
    "    \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt_template),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0b8d9dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:01:01.203731Z",
     "iopub.status.busy": "2024-06-16T11:01:01.203387Z",
     "iopub.status.idle": "2024-06-16T11:01:31.090223Z",
     "shell.execute_reply": "2024-06-16T11:01:31.089245Z",
     "shell.execute_reply.started": "2024-06-16T11:01:01.203707Z"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your question\n",
      " explain how the transformers works?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer\n",
      " The Transformer is a machine learning model architecture that has revolutionized the field of natural language processing (NLP). It was introduced in a seminal paper titled \"Attention is All You Need\" by Vaswani et al. in 2017. The key innovation of the Transformer is its use of self-attention mechanisms and fully connected layers to process sequential data, such as text, in parallel.\n",
      "\n",
      "Here's a high-level explanation of how the Transformer works:\n",
      "1. Input Embeddings: The input to the Transformer model is typically a sequence of tokens, such as words or subwords. These tokens are first passed through an embedding layer, which converts them into dense vector representations.\n",
      "2. Encoder:\n",
      "   - The encoder is responsible for transforming the input sequence into a new representation that captures the contextual relationships between the tokens.\n",
      "   - It is composed of a stack of identical layers, each containing two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network.\n",
      "   - The multi-head self-attention mechanism allows the model to focus on different aspects of the input sequence simultaneously, enabling it to capture complex dependencies between tokens.\n",
      "   - Each layer in the encoder also includes a residual connection and layer normalization to facilitate training and improve performance.\n",
      "3. Decoder:\n",
      "   - The decoder generates the output sequence one token at a time, conditioned on the encoded input and the previously generated tokens.\n",
      "   - Similar to the encoder, the decoder also consists of a stack of identical layers, each with three sub-layers: a multi-head self-attention mechanism, an encoder-decoder attention mechanism, and a position-wise fully connected feed-forward network.\n",
      "   - The multi-head self-attention mechanism in the decoder allows it to attend to different positions in the decoder's own output sequence.\n",
      "   - The encoder-decoder attention mechanism enables the decoder to focus on relevant parts of the input sequence while generating the output.\n",
      "4. Attention Mechanisms:\n",
      "   - Self-attention: This allows the model to weigh the importance of different tokens in the input sequence relative to each other, capturing dependencies without regard to their distance in the sequence.\n",
      "   - Encoder-decoder attention: This allows the decoder to attend to all positions in the input sequence, enabling it to generate output tokens that are contextually relevant.\n",
      "5. Position Encoding:\n",
      "   - Unlike traditional recurrent neural networks (RNNs), the Transformer does not process the input sequence sequentially. Instead, it relies on position encodings, which are added to the input embeddings, to provide information about the relative positions of the tokens in the sequence.\n",
      "6. Output Generation:\n",
      "   - The decoder generates the output sequence one token at a time. At each step, it uses the encoded input, the previously generated tokens, and the attention mechanisms to predict the next token.\n",
      "\n",
      "The Transformer's ability to process input sequences in parallel and its heavy use of attention mechanisms make it highly effective and efficient for tasks such as machine translation, text generation, sentiment analysis, and many other NLP applications. It has sparked a wave of research and innovation in the field of NLP, leading to the development of numerous variants and improvements.\n"
     ]
    }
   ],
   "source": [
    "question=input(\"Enter your question\\n\")\n",
    "context=retriever.invoke(question)\n",
    "answer=chain.invoke({\"input\": question,\"context\":context})\n",
    "print(\"The answer\\n\",answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c394bdd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:23:26.108637Z",
     "iopub.status.busy": "2024-06-16T11:23:26.107971Z",
     "iopub.status.idle": "2024-06-16T11:23:26.113550Z",
     "shell.execute_reply": "2024-06-16T11:23:26.112609Z",
     "shell.execute_reply.started": "2024-06-16T11:23:26.108602Z"
    }
   },
   "outputs": [],
   "source": [
    "#generaed answer\n",
    "gen_answers=answer.content\n",
    "#reference answer\n",
    "ref_answer=\"\"\n",
    "for i in range(len(context)):\n",
    "    content=context[i].page_content\n",
    "    ref_answer+=content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8460e0",
   "metadata": {},
   "source": [
    "# **# Evalution of the LLM Generated Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0391f",
   "metadata": {},
   "source": [
    "# Calculate ROUGE score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f66c096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:19:26.448960Z",
     "iopub.status.busy": "2024-06-16T11:19:26.448243Z",
     "iopub.status.idle": "2024-06-16T11:19:26.458461Z",
     "shell.execute_reply": "2024-06-16T11:19:26.457417Z",
     "shell.execute_reply.started": "2024-06-16T11:19:26.448926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 score: 0.2397003745318352\n"
     ]
    }
   ],
   "source": [
    " def calculate_ROUGE(generated_summary, reference_summary, n):\n",
    "    # Tokenize the generated summary and reference summary into n-grams\n",
    "    generated_ngrams = generate_ngrams(generated_summary, n)\n",
    "    reference_ngrams = generate_ngrams(reference_summary, n)\n",
    " \n",
    "    # Calculate the recall score\n",
    "    matching_ngrams = len(set(generated_ngrams) & set(reference_ngrams))\n",
    "    recall_score = matching_ngrams / len(reference_ngrams)\n",
    " \n",
    "    return recall_score\n",
    " \n",
    " \n",
    "def generate_ngrams(text, n):\n",
    "    # Preprocess text by removing punctuation and converting to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    " \n",
    "    # Generate n-grams from the preprocessed text\n",
    "    words = text.split()\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    " \n",
    "    return ngrams\n",
    " \n",
    " \n",
    "\n",
    "generated_summary = gen_answers\n",
    "reference_summary = ref_answer\n",
    "n = 1  \n",
    "rouge_score = calculate_ROUGE(generated_summary, reference_summary, n)\n",
    "print(f\"ROUGE-{n} score: {rouge_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a01c8",
   "metadata": {},
   "source": [
    "# Calculate the BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2bfc06f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T11:19:17.198108Z",
     "iopub.status.busy": "2024-06-16T11:19:17.197223Z",
     "iopub.status.idle": "2024-06-16T11:19:17.205974Z",
     "shell.execute_reply": "2024-06-16T11:19:17.204895Z",
     "shell.execute_reply.started": "2024-06-16T11:19:17.198073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1 score: 0.2777777777777778\n"
     ]
    }
   ],
   "source": [
    "def calculate_BLEU(generated_summary, reference_summary, n):\n",
    "    # Tokenize the generated summary and reference summary\n",
    "    generated_tokens = generated_summary.split()\n",
    "    reference_tokens = reference_summary.split()\n",
    " \n",
    "    # Calculate the BLEU score\n",
    "    weights = [1.0 / n] * n  # Weights for n-gram precision calculation\n",
    "    bleu_score = bleu.sentence_bleu([reference_tokens], generated_tokens, weights=weights)\n",
    " \n",
    "    return bleu_score\n",
    " \n",
    " \n",
    "\n",
    "generated_summary = gen_answers\n",
    "reference_summary = ref_answer\n",
    "n = 1\n",
    " \n",
    "bleu_score = calculate_BLEU(generated_summary, reference_summary, n)\n",
    "print(f\"BLEU-{n} score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5aa12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5218337,
     "sourceId": 8700712,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5218352,
     "sourceId": 8700734,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5218401,
     "sourceId": 8700808,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5218405,
     "sourceId": 8700815,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
